{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from joblib import dump, load\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# 忽略 ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "    doc = doc.replace(b\"<br />\", b\" \")  # 替换换行符\n",
    "    doc = re.sub(b\"<.*?>\", b\" \", doc)  # 移除 HTML 标签\n",
    "    doc = re.sub(b\"[^\\w\\s]\", b\" \", doc)  # 移除标点符号\n",
    "    doc = re.sub(b\"\\d+\", b\" \", doc)  # 移除数字\n",
    "    doc = re.sub(b\"\\s+\", b\" \", doc).strip()  # 移除多余空格\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_topics = 15  # 主题数量\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=5,\n",
    "    learning_method='online',\n",
    "    learning_offset=50.,\n",
    "    random_state=0\n",
    ")\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=15, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    # 获取主题数量\n",
    "    n_topics = len(model.components_)\n",
    "    \n",
    "    # 每行显示的主题数\n",
    "    topics_per_row = 5\n",
    "    \n",
    "    # 遍历每组主题\n",
    "    for group in range(0, n_topics, topics_per_row):\n",
    "        # 当前组的主题数\n",
    "        current_topics = min(topics_per_row, n_topics - group)\n",
    "        \n",
    "        # 打印表头\n",
    "        topics = [f\"topic {i}\" for i in range(group, group + current_topics)]\n",
    "        header = \"     \".join(topics)\n",
    "        print(\"\\n\" + header)\n",
    "        print(\"--------    \" * current_topics)\n",
    "        \n",
    "        # 获取当前组主题的关键词\n",
    "        top_words = []\n",
    "        for topic_idx in range(group, group + current_topics):\n",
    "            topic = model.components_[topic_idx]\n",
    "            top_indices = topic.argsort()[:-n_top_words-1:-1]\n",
    "            top_words.append([feature_names[i] for i in top_indices])\n",
    "        \n",
    "        # 按行打印词\n",
    "        for i in range(n_top_words):\n",
    "            row = []\n",
    "            for topic_words in top_words:\n",
    "                row.append(f\"{topic_words[i]:<12}\")\n",
    "            print(\"\".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "topic 0     topic 1     topic 2     topic 3     topic 4\n",
      "--------    --------    --------    --------    --------    \n",
      "didn        music       comedy      role        sex         \n",
      "thought     musical     funny       john        women       \n",
      "thing       songs       fun         performance tom         \n",
      "lot         song        zombie      cast        american    \n",
      "though      dance       horror      play        black       \n",
      "doesn       dancing     house       actor       woman       \n",
      "want        singing     dr          played      around      \n",
      "going       kelly       comedies    mr          another     \n",
      "10          number      humor       james       christmas   \n",
      "things      numbers     doctor      british     white       \n",
      "re          allen       afraid      plays       house       \n",
      "real        band        hilarious   new         our         \n",
      "actually    stage       laughs      young       doesn       \n",
      "few         voice       grant       jack        three       \n",
      "every       tarzan      werewolf    ben         own         \n",
      "\n",
      "topic 5     topic 6     topic 7     topic 8     topic 9\n",
      "--------    --------    --------    --------    --------    \n",
      "horror      worst       cartoon     war         action      \n",
      "effects     awful       de          political   police      \n",
      "gore        script      sucks       us          fight       \n",
      "blood       waste       ship        church      hero        \n",
      "scary       nothing     cartoons    world       lee         \n",
      "pretty      terrible    columbo     german      cop         \n",
      "killer      budget      caine       south       car         \n",
      "dead        minutes     bourne      history     guy         \n",
      "gets        low         titanic     government  charlie     \n",
      "special     stupid      davis       god         gets        \n",
      "night       money       lisa        anti        fighting    \n",
      "monster     actors      hole        christian   gun         \n",
      "evil        crap        stanwyck    against     martial     \n",
      "around      boring      nancy       hitler      keaton      \n",
      "slasher     horrible    inspector   religious   arts        \n",
      "\n",
      "topic 10     topic 11     topic 12     topic 13     topic 14\n",
      "--------    --------    --------    --------    --------    \n",
      "steve       show        michael     world       young       \n",
      "match       series      sci         action      father      \n",
      "jerry       tv          fi          quite       family      \n",
      "bob         episode     rock        original    woman       \n",
      "anna        old         star        beautiful   wife        \n",
      "sean        family      jackson     animation   between     \n",
      "dan         kids        chris       years       director    \n",
      "eric        funny       bruce       game        seems       \n",
      "ted         years       arthur      work        murder      \n",
      "che         episodes    jim         special     mother      \n",
      "superman    shows       welles      new         own         \n",
      "reunion     now         kate        both        both        \n",
      "ring        season      oscar       cinema      son         \n",
      "elvira      television  sam         effects     makes       \n",
      "dave        year        moore       art         takes       \n"
     ]
    }
   ],
   "source": [
    "# 显示每个主题的前15个关键词\n",
    "n_top_words = 15\n",
    "print_top_words(lda, vect.get_feature_names_out(), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文档-主题分布\n",
    "doc_topics = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 15)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印每个文档最主要的主题\n",
    "def print_document_topics(doc_topics, n_docs=15):\n",
    "    print(f\"\\n前{n_docs}个文档的主题分布:\")\n",
    "    for i in range(min(n_docs, len(doc_topics))):\n",
    "        topic_distribution = doc_topics[i]\n",
    "        dominant_topic = topic_distribution.argmax()\n",
    "        print(f\"文档 #{i + 1}: 主要属于主题 {dominant_topic + 1} (概率: {topic_distribution[dominant_topic]:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "前15个文档的主题分布:\n",
      "文档 #1: 主要属于主题 15 (概率: 0.80)\n",
      "文档 #2: 主要属于主题 1 (概率: 0.77)\n",
      "文档 #3: 主要属于主题 1 (概率: 0.70)\n",
      "文档 #4: 主要属于主题 1 (概率: 0.41)\n",
      "文档 #5: 主要属于主题 1 (概率: 0.44)\n",
      "文档 #6: 主要属于主题 13 (概率: 0.54)\n",
      "文档 #7: 主要属于主题 1 (概率: 0.98)\n",
      "文档 #8: 主要属于主题 1 (概率: 0.58)\n",
      "文档 #9: 主要属于主题 15 (概率: 0.58)\n",
      "文档 #10: 主要属于主题 4 (概率: 0.46)\n",
      "文档 #11: 主要属于主题 10 (概率: 0.62)\n",
      "文档 #12: 主要属于主题 7 (概率: 0.76)\n",
      "文档 #13: 主要属于主题 10 (概率: 0.48)\n",
      "文档 #14: 主要属于主题 4 (概率: 0.40)\n",
      "文档 #15: 主要属于主题 3 (概率: 0.78)\n"
     ]
    }
   ],
   "source": [
    "print_document_topics(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mapping = {\n",
    "    0: \"观后感\",\n",
    "    1: \"音乐舞蹈\",\n",
    "    2: \"喜剧\",\n",
    "    3: \"表演\",\n",
    "    4: \"社会议题\",\n",
    "    5: \"恐怖惊悚\",\n",
    "    6: \"负面评价\",\n",
    "    7: \"经典电影\",\n",
    "    8: \"政治历史\",\n",
    "    9: \"动作警匪\",\n",
    "    10: \"人物\",\n",
    "    11: \"电视\",\n",
    "    12: \"科幻\",\n",
    "    13: \"视觉\",\n",
    "    14: \"家庭\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic(text, lda_model, vectorizer):\n",
    "\n",
    "    # 清理和预处理文本\n",
    "    cleaned_text = clean_text(text.encode())\n",
    "    \n",
    "    # 转换文本为向量\n",
    "    text_vector = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # 预测主题分布\n",
    "    topic_dist = lda_model.transform(text_vector)[0]\n",
    "    \n",
    "    # 获取主要主题\n",
    "    main_topic = topic_dist.argmax()\n",
    "    \n",
    "    print(f\"\\n最可能的主题是: {topic_mapping[main_topic]} (Topic {main_topic}, 概率: {topic_dist[main_topic]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最可能的主题是: 喜剧 (Topic 2, 概率: 0.867)\n"
     ]
    }
   ],
   "source": [
    "# 使用示例\n",
    "test_text = \"This movie was really funny and made me laugh a lot. The comedy was great and the jokes were hilarious.\"\n",
    "predict_topic(test_text, lda, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlda = load('models/lda_model.joblib')\\nvect = load('models/vectorizer.joblib')\\ntopic_mapping = load('models/topic_mapping.joblib')\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存到models文件夹\n",
    "dump(lda, 'models3/lda_model.joblib')\n",
    "dump(vect, 'models3/vectorizer.joblib')\n",
    "dump(topic_mapping, 'models3/topic_mapping.joblib')\n",
    "\n",
    "\n",
    "# 加载示例：\n",
    "'''\n",
    "lda = load('models/lda_model.joblib')\n",
    "vect = load('models/vectorizer.joblib')\n",
    "topic_mapping = load('models/topic_mapping.joblib')\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "MLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
